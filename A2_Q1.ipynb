{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of this assignment is to get you familiarize with  the  problem  of  `Clustering`.\n",
    "\n",
    "## Instructions\n",
    "- Write your code and analysis in the indicated cells.\n",
    "- Ensure that this notebook runs without errors when the cells are run in sequence.\n",
    "- Do not attempt to change the contents of other cells.\n",
    "- No inbuilt functions to be used until specified\n",
    "\n",
    "## Submission\n",
    "- Ensure that this notebook runs without errors when the cells are run in sequence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TdEXGGO0AqkW",
    "outputId": "119fb7e5-3656-4bdb-fcfa-2e33afe2d0d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')    \n",
    "# if u r facing issues while importing nltk, please uncomment above line and run\n",
    "import re\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5N2FkdFMxixH"
   },
   "outputs": [],
   "source": [
    "!pip install sentence-transformers\n",
    "from sentence_transformers import SentenceTransformer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s-1irtUhxlp2"
   },
   "source": [
    "# Dataset\n",
    "\n",
    "\n",
    "\n",
    "*   Try to explore the dataset and first understand\n",
    "*   Steps while processing the dataset:\n",
    "\n",
    "1.   Load the dataset\n",
    ">> The 20 newsgroups dataset comprises around 18000 newsgroups posts on 20 topics split in two subsets: train and test. Here, we only use train part of the dataset as we don't need any training.\n",
    "\n",
    "2.   pre-processing of the dataset\n",
    ">>   A set of basic pre-processing steps are given below, if you can do it better, it is appreciable\n",
    "3.   Trying to obtain the embeddings for the text. \n",
    ">> Here, we used bert model to obtain the embeddings, if you want to use anyother sentence/word embeddings (ELMo,universal sentence encoder, or other bert models) you can use it, but not mandatorily change it)\n",
    "\n",
    "PS: You need not completely understand how bert works. If you are interested, few links will be mentioned below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oqkn6iSNmEo1"
   },
   "outputs": [],
   "source": [
    "# loading of dataset\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroups_train = fetch_20newsgroups(subset='train')\n",
    "\n",
    "# print(list(newsgroups_train))\n",
    "#['data', 'filenames', 'target_names', 'target', 'DESCR']\n",
    "# all we require for our task is data and target. \n",
    "#target_names describe the different groups present (which are 20) all over the dataset\n",
    "\n",
    "# print(list(newsgroups_train.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NNRbIfsX01hD"
   },
   "outputs": [],
   "source": [
    "bert_model = SentenceTransformer('bert-base-nli-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kp8UL8lP5U2k"
   },
   "outputs": [],
   "source": [
    "#preprocessing of sentences and the article\n",
    "\n",
    "def remove_punct(text):\n",
    "  text = re.sub('[^a-zA-Z0-9 ]+','', text)\n",
    "  return text\n",
    "\n",
    "def remove_urls(text):\n",
    "  url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "  return url_pattern.sub(r'', text)\n",
    "\n",
    "def remove_tag(text):   \n",
    "  text=' '.join(text)\n",
    "  html_pattern = re.compile('<.*?>')\n",
    "  return html_pattern.sub(r'', text)\n",
    "\n",
    "def pre_process_sentence(sentence):\n",
    "  sentence = sentence.lower()\n",
    "  sentence = remove_punct(remove_urls(sentence))\n",
    "  return sentence\n",
    "\n",
    "def pre_process_article(article):\n",
    "  article = str(article).replace(\"\\n\", '')\n",
    "  article = sent_tokenize(article)\n",
    "  sentences = []\n",
    "  for each in article:\n",
    "    if len(each.split(\":\")) > 1:\n",
    "      continue\n",
    "    sentences.append(pre_process_sentence(each))\n",
    "  return sentences\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "72IcGYs980zK"
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_review_embedding(article):\n",
    "  sentences = pre_process_article(article)\n",
    "\n",
    "  #here review(input) has to be a list of sentences\n",
    "  #use suitable embeddings to get an embedding for the whole review\n",
    "  #usage of sentence embeddings is recommended\n",
    "\n",
    "  sentence_embeddings = bert_model.encode(sentences)\n",
    "\n",
    "  # take average of all sentence embeddings to obtain a review embedding \n",
    "  review_embedding = np.zeros(768)\n",
    "  for each in sentence_embeddings:\n",
    "    review_embedding = np.add(np.array(each), review_embedding)\n",
    "\n",
    "  return review_embedding\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pbkl4Ux0JPi2"
   },
   "outputs": [],
   "source": [
    "X, y = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JkaKG4WW9eXx"
   },
   "outputs": [],
   "source": [
    "# data visualization \n",
    "\n",
    "# Try to visualise the points from all the domains and try to visualise them \n",
    "# hint: you can use PCA \n",
    "\n",
    "import sklearn\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uZH6gO7HA5jk"
   },
   "source": [
    "# K_Means Algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T9TBSHWfgWxU",
    "outputId": "8d4ab613-7513-4732-bf06-8b7e7cd8a2c9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# code to write your Kmeans algorithm\n",
    "#implement your KMeans algorithm here, and visualise the clusters obtained \n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=2, random_state=4).fit(X)\n",
    "kmeans.labels_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "61GQRof2wkKd"
   },
   "outputs": [],
   "source": [
    "#code for visualisation of clusters\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m2t9Cz6yuJmO"
   },
   "source": [
    "# Elbow method\n",
    "\n",
    "\n",
    "\n",
    "*   Try to understand how elbow method works\n",
    "*   Plot the graph between average distance and the number of clusters\n",
    "*   Use elbow method to find the optimal number of clusters, \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JW_stdJuub4s"
   },
   "outputs": [],
   "source": [
    "def elbow_method():\n",
    "\n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2jHeGh-xuuDc"
   },
   "source": [
    "# Silhouette Method\n",
    "\n",
    "\n",
    "*   Compute silhouette score varying the K number of clusters\n",
    "\n",
    "*   Plot the graph between silhoutte score and number of clusters \n",
    "\n",
    "*   Find the optimal number of clusters using silhouette method\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "> Report the optimal number of clusters you obtained from above two methods (elbow and silhouette)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lKRJ4IBMuwfd"
   },
   "outputs": [],
   "source": [
    "def silhouette_score():\n",
    "\n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JEvLJxmnuCof"
   },
   "source": [
    "# Agglomerative clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nzssThP1g_NN",
    "outputId": "27d005d2-7405-4a81-a644-f35bdd3ef4f1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# code to write your Kmeans algorithm\n",
    "#implement your KMeans algorithm here, and visualise the clusters obtained \n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "def AgglomerativeClustering():\n",
    "  clustering = AgglomerativeClustering().fit(visuals)\n",
    "  clustering.labels_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "whf5OLe2wf6o"
   },
   "outputs": [],
   "source": [
    "#code for visualisation of clusters\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZdNC7XxxwuJW"
   },
   "source": [
    "# Dendogram\n",
    "\n",
    "\n",
    "*   Try to understand the difference between agglomerative clustering and hierarchical clustering\n",
    "*   Plot dendograms for both kinds of clustering\n",
    "*   Find the optimal number of clusters with the help of Dendogram\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1NWavQn0hW3i"
   },
   "outputs": [],
   "source": [
    "# code to write dendogram\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B3On7lpA11eB"
   },
   "source": [
    "# useful links to understand BERT\n",
    "\n",
    "*  https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270#:~:text=How%20BERT%20works,%2Dwords)%20in%20a%20text.&text=As%20opposed%20to%20directional%20models,sequence%20of%20words%20at%20once.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Q_Clustering.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
